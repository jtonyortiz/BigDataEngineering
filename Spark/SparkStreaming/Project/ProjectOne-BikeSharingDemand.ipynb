{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bike Sharing Demand Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Exploration and Transformation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datafields:\n",
    "- datetime - hourly date + timestamp  \n",
    "- season -  1 = spring, 2 = summer, 3 = fall, 4 = winter \n",
    "- holiday - whether the day is considered a holiday\n",
    "- workingday - whether the day is neither a weekend nor holiday\n",
    "- weather - 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \n",
    "- temp - temperature in Celsius\n",
    "- atemp - \"feels like\" temperature in Celsius\n",
    "- humidity - relative humidity\n",
    "- windspeed - wind speed\n",
    "- casual - number of non-registered user rentals initiated\n",
    "- registered - number of registered user rentals initiated\n",
    "- count - number of total rentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master('yarn').appName('RDD Creation').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.options(header='True', inferSchema='True', delimiter=',').csv(\"pyspark-project1/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datetime: timestamp (nullable = true)\n",
      " |-- season: integer (nullable = true)\n",
      " |-- holiday: integer (nullable = true)\n",
      " |-- workingday: integer (nullable = true)\n",
      " |-- weather: integer (nullable = true)\n",
      " |-- temp: double (nullable = true)\n",
      " |-- atemp: double (nullable = true)\n",
      " |-- humidity: integer (nullable = true)\n",
      " |-- windspeed: double (nullable = true)\n",
      " |-- casual: integer (nullable = true)\n",
      " |-- registered: integer (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+-------+----------+-------+-----+------+--------+---------+------+----------+-----+\n",
      "|           datetime|season|holiday|workingday|weather| temp| atemp|humidity|windspeed|casual|registered|count|\n",
      "+-------------------+------+-------+----------+-------+-----+------+--------+---------+------+----------+-----+\n",
      "|2011-01-01 00:00:00|     1|      0|         0|      1| 9.84|14.395|      81|      0.0|     3|        13|   16|\n",
      "|2011-01-01 01:00:00|     1|      0|         0|      1| 9.02|13.635|      80|      0.0|     8|        32|   40|\n",
      "|2011-01-01 02:00:00|     1|      0|         0|      1| 9.02|13.635|      80|      0.0|     5|        27|   32|\n",
      "|2011-01-01 03:00:00|     1|      0|         0|      1| 9.84|14.395|      75|      0.0|     3|        10|   13|\n",
      "|2011-01-01 04:00:00|     1|      0|         0|      1| 9.84|14.395|      75|      0.0|     0|         1|    1|\n",
      "|2011-01-01 05:00:00|     1|      0|         0|      2| 9.84| 12.88|      75|   6.0032|     0|         1|    1|\n",
      "|2011-01-01 06:00:00|     1|      0|         0|      1| 9.02|13.635|      80|      0.0|     2|         0|    2|\n",
      "|2011-01-01 07:00:00|     1|      0|         0|      1|  8.2| 12.88|      86|      0.0|     1|         2|    3|\n",
      "|2011-01-01 08:00:00|     1|      0|         0|      1| 9.84|14.395|      75|      0.0|     1|         7|    8|\n",
      "|2011-01-01 09:00:00|     1|      0|         0|      1|13.12|17.425|      76|      0.0|     8|         6|   14|\n",
      "+-------------------+------+-------+----------+-------+-----+------+--------+---------+------+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+-------+----------+-------+----+------+--------+---------+------+----------+-----+\n",
      "|           datetime|season|holiday|workingday|weather|temp| atemp|humidity|windspeed|casual|registered|count|\n",
      "+-------------------+------+-------+----------+-------+----+------+--------+---------+------+----------+-----+\n",
      "|2011-01-01 00:00:00|     1|      0|         0|      1|9.84|14.395|      81|      0.0|     3|        13|   16|\n",
      "|2011-01-01 01:00:00|     1|      0|         0|      1|9.02|13.635|      80|      0.0|     8|        32|   40|\n",
      "|2011-01-01 02:00:00|     1|      0|         0|      1|9.02|13.635|      80|      0.0|     5|        27|   32|\n",
      "|2011-01-01 03:00:00|     1|      0|         0|      1|9.84|14.395|      75|      0.0|     3|        10|   13|\n",
      "|2011-01-01 04:00:00|     1|      0|         0|      1|9.84|14.395|      75|      0.0|     0|         1|    1|\n",
      "+-------------------+------+-------+----------+-------+----+------+--------+---------+------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.show(5)) #Summarize Table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) FileScan csv [datetime#225,season#226,holiday#227,workingday#228,weather#229,temp#230,atemp#231,humidity#232,windspeed#233,casual#234,registered#235,count#236] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://nameservice1/user/educlbdhid0017/pyspark-project1/train.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<datetime:timestamp,season:int,holiday:int,workingday:int,weather:int,temp:double,atemp:dou...\n"
     ]
    }
   ],
   "source": [
    "df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for missing values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10886\n",
      "10886\n"
     ]
    }
   ],
   "source": [
    "print(df.count()) #Find the number of values\n",
    "df_no_null = df.na.drop() #Drop all NULL/NAN values\n",
    "print(df_no_null.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the number of seasons found ih the dataset and explode them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[season: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.select('season').distinct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating new columns in Dataset:\n",
    "def valueToCategory(value, encoding_index):\n",
    "   if(value == encoding_index):\n",
    "      return 1\n",
    "   else:\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explode season column into seperate columns and drop season "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col\n",
    "udfValueToCategory = udf(valueToCategory, IntegerType())\n",
    "df_encoded = (df.withColumn(\"season_1\", udfValueToCategory(col('season'),lit(1)))\n",
    "                     .withColumn(\"season_2\", udfValueToCategory(col('season'),lit(2)))\n",
    "                     .withColumn(\"season_3\", udfValueToCategory(col('season'),lit(3)))\n",
    "                     .withColumn(\"season_4\", udfValueToCategory(col('season'),lit(4))))\n",
    "#Drop Season Column\n",
    "df_encoded = df_encoded.drop('season')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(datetime=datetime.datetime(2011, 1, 1, 0, 0), holiday=0, workingday=0, weather=1, temp=9.84, atemp=14.395, humidity=81, windspeed=0.0, casual=3, registered=13, count=16, season_1=1, season_2=0, season_3=0, season_4=0),\n",
       " Row(datetime=datetime.datetime(2011, 1, 1, 1, 0), holiday=0, workingday=0, weather=1, temp=9.02, atemp=13.635, humidity=80, windspeed=0.0, casual=8, registered=32, count=40, season_1=1, season_2=0, season_3=0, season_4=0),\n",
       " Row(datetime=datetime.datetime(2011, 1, 1, 2, 0), holiday=0, workingday=0, weather=1, temp=9.02, atemp=13.635, humidity=80, windspeed=0.0, casual=5, registered=27, count=32, season_1=1, season_2=0, season_3=0, season_4=0),\n",
       " Row(datetime=datetime.datetime(2011, 1, 1, 3, 0), holiday=0, workingday=0, weather=1, temp=9.84, atemp=14.395, humidity=75, windspeed=0.0, casual=3, registered=10, count=13, season_1=1, season_2=0, season_3=0, season_4=0),\n",
       " Row(datetime=datetime.datetime(2011, 1, 1, 4, 0), holiday=0, workingday=0, weather=1, temp=9.84, atemp=14.395, humidity=75, windspeed=0.0, casual=0, registered=1, count=1, season_1=1, season_2=0, season_3=0, season_4=0)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+----------+-------+----+------+--------+---------+------+----------+-----+--------+--------+--------+--------+\n",
      "|           datetime|holiday|workingday|weather|temp| atemp|humidity|windspeed|casual|registered|count|season_1|season_2|season_3|season_4|\n",
      "+-------------------+-------+----------+-------+----+------+--------+---------+------+----------+-----+--------+--------+--------+--------+\n",
      "|2011-01-01 00:00:00|      0|         0|      1|9.84|14.395|      81|      0.0|     3|        13|   16|       1|       0|       0|       0|\n",
      "|2011-01-01 01:00:00|      0|         0|      1|9.02|13.635|      80|      0.0|     8|        32|   40|       1|       0|       0|       0|\n",
      "|2011-01-01 02:00:00|      0|         0|      1|9.02|13.635|      80|      0.0|     5|        27|   32|       1|       0|       0|       0|\n",
      "|2011-01-01 03:00:00|      0|         0|      1|9.84|14.395|      75|      0.0|     3|        10|   13|       1|       0|       0|       0|\n",
      "|2011-01-01 04:00:00|      0|         0|      1|9.84|14.395|      75|      0.0|     0|         1|    1|       1|       0|       0|       0|\n",
      "+-------------------+-------+----------+-------+----+------+--------+---------+------+----------+-----+--------+--------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_encoded.show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|weather|\n",
      "+-------+\n",
      "|      1|\n",
      "|      1|\n",
      "|      1|\n",
      "|      1|\n",
      "|      1|\n",
      "|      2|\n",
      "|      1|\n",
      "|      1|\n",
      "|      1|\n",
      "|      1|\n",
      "|      1|\n",
      "|      1|\n",
      "|      1|\n",
      "|      2|\n",
      "|      2|\n",
      "|      2|\n",
      "|      2|\n",
      "|      2|\n",
      "|      3|\n",
      "|      3|\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#display weather column:\n",
    "display(df.select('weather').show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = (df_encoded.withColumn(\"weather_1\", udfValueToCategory(col('weather'),lit(1)))\n",
    "                     .withColumn(\"weather_2\", udfValueToCategory(col('weather'),lit(2)))\n",
    "                     .withColumn(\"weather_3\", udfValueToCategory(col('weather'),lit(3)))\n",
    "                     .withColumn(\"weather_4\", udfValueToCategory(col('weather'),lit(4))))\n",
    "df_encoded = df_encoded.drop('weather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(datetime=datetime.datetime(2011, 1, 1, 0, 0), holiday=0, workingday=0, temp=9.84, atemp=14.395, humidity=81, windspeed=0.0, casual=3, registered=13, count=16, season_1=1, season_2=0, season_3=0, season_4=0, weather_1=1, weather_2=0, weather_3=0, weather_4=0),\n",
       " Row(datetime=datetime.datetime(2011, 1, 1, 1, 0), holiday=0, workingday=0, temp=9.02, atemp=13.635, humidity=80, windspeed=0.0, casual=8, registered=32, count=40, season_1=1, season_2=0, season_3=0, season_4=0, weather_1=1, weather_2=0, weather_3=0, weather_4=0),\n",
       " Row(datetime=datetime.datetime(2011, 1, 1, 2, 0), holiday=0, workingday=0, temp=9.02, atemp=13.635, humidity=80, windspeed=0.0, casual=5, registered=27, count=32, season_1=1, season_2=0, season_3=0, season_4=0, weather_1=1, weather_2=0, weather_3=0, weather_4=0),\n",
       " Row(datetime=datetime.datetime(2011, 1, 1, 3, 0), holiday=0, workingday=0, temp=9.84, atemp=14.395, humidity=75, windspeed=0.0, casual=3, registered=10, count=13, season_1=1, season_2=0, season_3=0, season_4=0, weather_1=1, weather_2=0, weather_3=0, weather_4=0),\n",
       " Row(datetime=datetime.datetime(2011, 1, 1, 4, 0), holiday=0, workingday=0, temp=9.84, atemp=14.395, humidity=75, windspeed=0.0, casual=0, registered=1, count=1, season_1=1, season_2=0, season_3=0, season_4=0, weather_1=1, weather_2=0, weather_3=0, weather_4=0)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split datetime into meaningful columns such as hour, day, month, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "df_encoded = df_encoded.withColumn('hour',  split(split(df_encoded['datetime'], ' ')[1], ':')[0].cast('int'))\n",
    "df_encoded = df_encoded.withColumn('month', split(split(df_encoded['datetime'], ' ')[0], '-')[0].cast('int'))\n",
    "df_encoded = df_encoded.withColumn('day', split(split(df_encoded['datetime'], ' ')[0], '-')[1].cast('int'))\n",
    "df_encoded = df_encoded.withColumn('year', split(split(df_encoded['datetime'], ' ')[0], '-')[2].cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(datetime=datetime.datetime(2011, 1, 1, 0, 0), holiday=0, workingday=0, temp=9.84, atemp=14.395, humidity=81, windspeed=0.0, casual=3, registered=13, count=16, season_1=1, season_2=0, season_3=0, season_4=0, weather_1=1, weather_2=0, weather_3=0, weather_4=0, hour=0, month=2011, day=1, year=1),\n",
       " Row(datetime=datetime.datetime(2011, 1, 1, 1, 0), holiday=0, workingday=0, temp=9.02, atemp=13.635, humidity=80, windspeed=0.0, casual=8, registered=32, count=40, season_1=1, season_2=0, season_3=0, season_4=0, weather_1=1, weather_2=0, weather_3=0, weather_4=0, hour=1, month=2011, day=1, year=1),\n",
       " Row(datetime=datetime.datetime(2011, 1, 1, 2, 0), holiday=0, workingday=0, temp=9.02, atemp=13.635, humidity=80, windspeed=0.0, casual=5, registered=27, count=32, season_1=1, season_2=0, season_3=0, season_4=0, weather_1=1, weather_2=0, weather_3=0, weather_4=0, hour=2, month=2011, day=1, year=1),\n",
       " Row(datetime=datetime.datetime(2011, 1, 1, 3, 0), holiday=0, workingday=0, temp=9.84, atemp=14.395, humidity=75, windspeed=0.0, casual=3, registered=10, count=13, season_1=1, season_2=0, season_3=0, season_4=0, weather_1=1, weather_2=0, weather_3=0, weather_4=0, hour=3, month=2011, day=1, year=1),\n",
       " Row(datetime=datetime.datetime(2011, 1, 1, 4, 0), holiday=0, workingday=0, temp=9.84, atemp=14.395, humidity=75, windspeed=0.0, casual=0, registered=1, count=1, season_1=1, season_2=0, season_3=0, season_4=0, weather_1=1, weather_2=0, weather_3=0, weather_4=0, hour=4, month=2011, day=1, year=1)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_encoded.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+----------+----+------+--------+---------+------+----------+-----+--------+--------+--------+--------+---------+---------+---------+---------+----+-----+---+----+\n",
      "|           datetime|holiday|workingday|temp| atemp|humidity|windspeed|casual|registered|count|season_1|season_2|season_3|season_4|weather_1|weather_2|weather_3|weather_4|hour|month|day|year|\n",
      "+-------------------+-------+----------+----+------+--------+---------+------+----------+-----+--------+--------+--------+--------+---------+---------+---------+---------+----+-----+---+----+\n",
      "|2011-01-01 00:00:00|      0|         0|9.84|14.395|      81|      0.0|     3|        13|   16|       1|       0|       0|       0|        1|        0|        0|        0|   0| 2011|  1|   1|\n",
      "|2011-01-01 01:00:00|      0|         0|9.02|13.635|      80|      0.0|     8|        32|   40|       1|       0|       0|       0|        1|        0|        0|        0|   1| 2011|  1|   1|\n",
      "|2011-01-01 02:00:00|      0|         0|9.02|13.635|      80|      0.0|     5|        27|   32|       1|       0|       0|       0|        1|        0|        0|        0|   2| 2011|  1|   1|\n",
      "|2011-01-01 03:00:00|      0|         0|9.84|14.395|      75|      0.0|     3|        10|   13|       1|       0|       0|       0|        1|        0|        0|        0|   3| 2011|  1|   1|\n",
      "|2011-01-01 04:00:00|      0|         0|9.84|14.395|      75|      0.0|     0|         1|    1|       1|       0|       0|       0|        1|        0|        0|        0|   4| 2011|  1|   1|\n",
      "+-------------------+-------+----------+----+------+--------+---------+------+----------+-----+--------+--------+--------+--------+---------+---------+---------+---------+----+-----+---+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_encoded.show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datetime: timestamp (nullable = true)\n",
      " |-- holiday: integer (nullable = true)\n",
      " |-- workingday: integer (nullable = true)\n",
      " |-- temp: double (nullable = true)\n",
      " |-- atemp: double (nullable = true)\n",
      " |-- humidity: integer (nullable = true)\n",
      " |-- windspeed: double (nullable = true)\n",
      " |-- casual: integer (nullable = true)\n",
      " |-- registered: integer (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      " |-- season_1: integer (nullable = true)\n",
      " |-- season_2: integer (nullable = true)\n",
      " |-- season_3: integer (nullable = true)\n",
      " |-- season_4: integer (nullable = true)\n",
      " |-- weather_1: integer (nullable = true)\n",
      " |-- weather_2: integer (nullable = true)\n",
      " |-- weather_3: integer (nullable = true)\n",
      " |-- weather_4: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_encoded.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = df_encoded.drop('datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded =  df_encoded.withColumnRenamed(\"count\", \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into train and train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "train, test = df_encoded.randomSplit([0.9, 0.1], seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assembling and Sending Features to the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembled columns 'hour', 'day' etc  to vector column 'features'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(holiday=0, workingday=0, temp=3.28, atemp=2.275, humidity=79, windspeed=31.0009, casual=0, registered=24, label=24, season_1=1, season_2=0, season_3=0, season_4=0, weather_1=0, weather_2=0, weather_3=1, weather_4=0, hour=1, month=2012, day=2, year=12, features=SparseVector(21, {2: 3.28, 3: 2.275, 4: 79.0, 5: 31.0009, 7: 24.0, 8: 24.0, 9: 1.0, 15: 1.0, 17: 1.0, 18: 2012.0, 19: 2.0, 20: 12.0})),\n",
       " Row(holiday=0, workingday=0, temp=3.28, atemp=3.79, humidity=53, windspeed=16.9979, casual=0, registered=26, label=26, season_1=1, season_2=0, season_3=0, season_4=0, weather_1=1, weather_2=0, weather_3=0, weather_4=0, hour=8, month=2012, day=2, year=12, features=SparseVector(21, {2: 3.28, 3: 3.79, 4: 53.0, 5: 16.9979, 7: 26.0, 8: 26.0, 9: 1.0, 13: 1.0, 17: 8.0, 18: 2012.0, 19: 2.0, 20: 12.0})),\n",
       " Row(holiday=0, workingday=0, temp=3.28, atemp=4.545, humidity=53, windspeed=12.998, casual=0, registered=1, label=1, season_1=1, season_2=0, season_3=0, season_4=0, weather_1=1, weather_2=0, weather_3=0, weather_4=0, hour=4, month=2011, day=1, year=9, features=SparseVector(21, {2: 3.28, 3: 4.545, 4: 53.0, 5: 12.998, 7: 1.0, 8: 1.0, 9: 1.0, 13: 1.0, 17: 4.0, 18: 2011.0, 19: 1.0, 20: 9.0})),\n",
       " Row(holiday=0, workingday=0, temp=3.28, atemp=4.545, humidity=53, windspeed=12.998, casual=0, registered=1, label=1, season_1=1, season_2=0, season_3=0, season_4=0, weather_1=1, weather_2=0, weather_3=0, weather_4=0, hour=5, month=2011, day=1, year=9, features=SparseVector(21, {2: 3.28, 3: 4.545, 4: 53.0, 5: 12.998, 7: 1.0, 8: 1.0, 9: 1.0, 13: 1.0, 17: 5.0, 18: 2011.0, 19: 1.0, 20: 9.0})),\n",
       " Row(holiday=0, workingday=0, temp=3.28, atemp=4.545, humidity=53, windspeed=12.998, casual=1, registered=5, label=6, season_1=1, season_2=0, season_3=0, season_4=0, weather_1=1, weather_2=0, weather_3=0, weather_4=0, hour=7, month=2011, day=1, year=9, features=DenseVector([0.0, 0.0, 3.28, 4.545, 53.0, 12.998, 1.0, 5.0, 6.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 7.0, 2011.0, 1.0, 9.0]))]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9797\n",
      "9797\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"holiday\",\"workingday\",\"temp\",\"atemp\",\"humidity\",\"windspeed\",\"casual\",\"registered\",\"label\",\"season_1\",\"season_2\",\"season_3\",\"season_4\",\"weather_1\",\"weather_2\",\"weather_3\",\"weather_4\", \"hour\", \"month\", \"day\", \"year\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "output = assembler.transform(train)\n",
    "print(\"Assembled columns 'hour', 'day' etc  to vector column 'features'\")\n",
    "display(output.take(5))\n",
    "print(output.count())\n",
    "train_output = output.na.drop()\n",
    "print(train_output.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1089\n",
      "1089\n",
      "Assembled columns 'hour', 'day' etc  to vector column 'features'\n"
     ]
    }
   ],
   "source": [
    "test_output = assembler.transform(test)\n",
    "print(test_output.count())\n",
    "train_output = test_output.na.drop()\n",
    "print(test_output.count())\n",
    "print(\"Assembled columns 'hour', 'day' etc  to vector column 'features'\")\n",
    "#select(\"features\", \"clicked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the Model\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(maxIter=10)\n",
    "lrModel = lr.fit(train_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.2410070188565101,0.024030060989444376,-0.003297725855819951,-0.003820121916425772,-0.0031189855279797744,0.0006257837922436448,0.5632573474293397,0.5631891304217548,0.43676973472610325,0.6470373361880243,0.23954646048626346,-0.17068049666052562,-0.686157845775815,-0.050530037272087616,0.0248775678319237,0.08457201334769567,0.0,0.004769967132704666,-0.08677748541178142,0.15866876140276176,0.005723255520058335]\n",
      "Intercept: 173.74350476370518\n"
     ]
    }
   ],
   "source": [
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=SparseVector(21, {2: 3.28, 3: 4.545, 4: 53.0, 5: 12.998, 7: 18.0, 8: 18.0, 9: 1.0, 13: 1.0, 17: 7.0, 18: 2012.0, 19: 2.0, 20: 12.0}), label=18, prediction=17.9770260555849),\n",
       " Row(features=SparseVector(21, {2: 4.1, 3: 3.03, 4: 39.0, 5: 30.0026, 7: 22.0, 8: 22.0, 9: 1.0, 13: 1.0, 17: 23.0, 18: 2011.0, 19: 1.0, 20: 8.0}), label=22, prediction=22.015787042195342),\n",
       " Row(features=SparseVector(21, {2: 5.74, 3: 7.575, 4: 43.0, 5: 11.0014, 7: 28.0, 8: 28.0, 9: 1.0, 13: 1.0, 17: 22.0, 18: 2012.0, 19: 2.0, 20: 12.0}), label=28, prediction=28.058417254402173),\n",
       " Row(features=DenseVector([0.0, 0.0, 6.56, 6.06, 40.0, 31.0009, 4.0, 92.0, 96.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 18.0, 2012.0, 2.0, 12.0]), label=96, prediction=96.06176875299458),\n",
       " Row(features=DenseVector([0.0, 0.0, 6.56, 6.82, 40.0, 22.0028, 4.0, 44.0, 48.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 18.0, 2011.0, 1.0, 9.0]), label=48, prediction=47.96614802554875),\n",
       " Row(features=DenseVector([0.0, 0.0, 6.56, 6.82, 47.0, 19.0012, 5.0, 38.0, 43.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 2.0, 2012.0, 1.0, 15.0]), label=43, prediction=42.88936084417972),\n",
       " Row(features=DenseVector([0.0, 0.0, 6.56, 6.82, 48.0, 26.0027, 1.0, 24.0, 25.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 3.0, 2012.0, 1.0, 15.0]), label=25, prediction=24.895860810314048),\n",
       " Row(features=DenseVector([0.0, 0.0, 6.56, 9.85, 59.0, 6.0032, 2.0, 18.0, 20.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 2011.0, 1.0, 15.0]), label=20, prediction=19.91497307956118),\n",
       " Row(features=SparseVector(21, {2: 6.56, 3: 9.85, 4: 69.0, 5: 6.0032, 6: 3.0, 7: 27.0, 8: 30.0, 9: 1.0, 13: 1.0, 18: 2011.0, 19: 2.0, 20: 12.0}), label=30, prediction=29.944761515373415),\n",
       " Row(features=SparseVector(21, {2: 6.56, 3: 11.365, 4: 59.0, 7: 1.0, 8: 1.0, 9: 1.0, 13: 1.0, 17: 5.0, 18: 2011.0, 19: 1.0, 20: 15.0}), label=1, prediction=0.8497462811985201)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Disply Predictions:\n",
    "import pyspark.sql.functions\n",
    "predictions = lrModel.transform(test_output)\\\n",
    "    .select(\"features\", \"label\", \"prediction\")\\\n",
    "    .take(10)\n",
    "display(predictions)\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"label\",metricName=\"r2\")\n",
    "# print(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=SparseVector(21, {2: 3.28, 3: 4.545, 4: 53.0, 5: 12.998, 7: 18.0, 8: 18.0, 9: 1.0, 13: 1.0, 17: 7.0, 18: 2012.0, 19: 2.0, 20: 12.0}), label=18, prediction=17.997756723800773),\n",
       " Row(features=SparseVector(21, {2: 4.1, 3: 3.03, 4: 39.0, 5: 30.0026, 7: 22.0, 8: 22.0, 9: 1.0, 13: 1.0, 17: 23.0, 18: 2011.0, 19: 1.0, 20: 8.0}), label=22, prediction=22.002205520528925),\n",
       " Row(features=SparseVector(21, {2: 5.74, 3: 7.575, 4: 43.0, 5: 11.0014, 7: 28.0, 8: 28.0, 9: 1.0, 13: 1.0, 17: 22.0, 18: 2012.0, 19: 2.0, 20: 12.0}), label=28, prediction=28.00228889226001),\n",
       " Row(features=DenseVector([0.0, 0.0, 6.56, 6.06, 40.0, 31.0009, 4.0, 92.0, 96.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 18.0, 2012.0, 2.0, 12.0]), label=96, prediction=95.99923333047346),\n",
       " Row(features=DenseVector([0.0, 0.0, 6.56, 6.82, 40.0, 22.0028, 4.0, 44.0, 48.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 18.0, 2011.0, 1.0, 9.0]), label=48, prediction=48.000842644425376)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Parameter grid search for best parameters to give good predictions\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "# TrainValidationSplit will try all combinations of values and determine best model using\n",
    "# the evaluator.\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(lr.fitIntercept, [False, True])\\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n",
    "    .build()\n",
    "\n",
    "# In this case the estimator is simply the linear regression.\n",
    "# A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "tvs = TrainValidationSplit(estimator=lr,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=RegressionEvaluator(),\n",
    "                           # 80% of the data will be used for training, 20% for validation.\n",
    "                           trainRatio=0.8)\n",
    "\n",
    "# Run TrainValidationSplit, and choose the best set of parameters.\n",
    "model = tvs.fit(train_output)\n",
    "\n",
    "# Make predictions on test data. model is the model with combination of parameters\n",
    "# that performed best.\n",
    "display(model.transform(test_output)\\\n",
    "    .select(\"features\", \"label\", \"prediction\")\\\n",
    "    .take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(prediction=34.931556750255645, label=18, features=SparseVector(21, {2: 3.28, 3: 4.545, 4: 53.0, 5: 12.998, 7: 18.0, 8: 18.0, 9: 1.0, 13: 1.0, 17: 7.0, 18: 2012.0, 19: 2.0, 20: 12.0})),\n",
       " Row(prediction=36.89780721865108, label=22, features=SparseVector(21, {2: 4.1, 3: 3.03, 4: 39.0, 5: 30.0026, 7: 22.0, 8: 22.0, 9: 1.0, 13: 1.0, 17: 23.0, 18: 2011.0, 19: 1.0, 20: 8.0})),\n",
       " Row(prediction=38.49619867538233, label=28, features=SparseVector(21, {2: 5.74, 3: 7.575, 4: 43.0, 5: 11.0014, 7: 28.0, 8: 28.0, 9: 1.0, 13: 1.0, 17: 22.0, 18: 2012.0, 19: 2.0, 20: 12.0})),\n",
       " Row(prediction=94.64301404077807, label=96, features=DenseVector([0.0, 0.0, 6.56, 6.06, 40.0, 31.0009, 4.0, 92.0, 96.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 18.0, 2012.0, 2.0, 12.0])),\n",
       " Row(prediction=57.55474515432686, label=48, features=DenseVector([0.0, 0.0, 6.56, 6.82, 40.0, 22.0028, 4.0, 44.0, 48.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 18.0, 2011.0, 1.0, 9.0]))]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 16.9584\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier model\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "rf = RandomForestRegressor(labelCol=\"label\", featuresCol=\"features\", numTrees=100)\n",
    "# Train model.  This also runs the indexers.\n",
    "rf_model = rf.fit(train_output)\n",
    "# rf_model.persist()\n",
    "# Make predictions.\n",
    "predictions = rf_model.transform(test_output)\n",
    "\n",
    "# Select example rows to display.\n",
    "display(predictions.select(\"prediction\", \"label\", \"features\").take(5))\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(prediction=16.889233655915504, label=18, features=SparseVector(21, {2: 3.28, 3: 4.545, 4: 53.0, 5: 12.998, 7: 18.0, 8: 18.0, 9: 1.0, 13: 1.0, 17: 7.0, 18: 2012.0, 19: 2.0, 20: 12.0})),\n",
       " Row(prediction=16.92511614166162, label=22, features=SparseVector(21, {2: 4.1, 3: 3.03, 4: 39.0, 5: 30.0026, 7: 22.0, 8: 22.0, 9: 1.0, 13: 1.0, 17: 23.0, 18: 2011.0, 19: 1.0, 20: 8.0})),\n",
       " Row(prediction=30.32523546505164, label=28, features=SparseVector(21, {2: 5.74, 3: 7.575, 4: 43.0, 5: 11.0014, 7: 28.0, 8: 28.0, 9: 1.0, 13: 1.0, 17: 22.0, 18: 2012.0, 19: 2.0, 20: 12.0})),\n",
       " Row(prediction=95.83936857912708, label=96, features=DenseVector([0.0, 0.0, 6.56, 6.06, 40.0, 31.0009, 4.0, 92.0, 96.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 18.0, 2012.0, 2.0, 12.0])),\n",
       " Row(prediction=46.025330660611566, label=48, features=DenseVector([0.0, 0.0, 6.56, 6.82, 40.0, 22.0028, 4.0, 44.0, 48.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 18.0, 2011.0, 1.0, 9.0]))]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 7.24449\n"
     ]
    }
   ],
   "source": [
    "#GBT Regressive Model\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "gbt = GBTRegressor(featuresCol=\"features\", maxIter=10)\n",
    "\n",
    "gbt_model = gbt.fit(train_output)\n",
    "# Make predictions.\n",
    "predictions = gbt_model.transform(test_output)\n",
    "\n",
    "\n",
    "gbt_model.write().overwrite().save(\"bike_sharing_gbt.model\")\n",
    "# Select example rows to display.\n",
    "display(predictions.select(\"prediction\", \"label\", \"features\").take(5))\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "#Gave root mean square error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
