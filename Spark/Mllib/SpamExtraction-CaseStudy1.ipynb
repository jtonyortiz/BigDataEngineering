{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master('yarn').appName('RDD creation').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|spam|             message|\n",
      "+----+--------------------+\n",
      "| ham|Go until jurong p...|\n",
      "| ham|Ok lar... Joking ...|\n",
      "+----+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw = spark.read.option(\"delimiter\",\"\\t\").csv(\"demos/SMSSpamCollection\").toDF(\"spam\",\"message\")\n",
    "raw.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+\n",
      "|spam|             message|               words|\n",
      "+----+--------------------+--------------------+\n",
      "| ham|Go until jurong p...|[go, until, juron...|\n",
      "+----+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract word\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "tokenizer = Tokenizer().setInputCol(\"message\").setOutputCol(\"words\")\n",
    "transformed = tokenizer.transform(raw)\n",
    "transformed.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+--------------------+\n",
      "|spam|             message|               words|            filtered|\n",
      "+----+--------------------+--------------------+--------------------+\n",
      "| ham|Go until jurong p...|[go, until, juron...|[go, jurong, poin...|\n",
      "+----+--------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "remover = StopWordsRemover().setInputCol(\"words\").setOutputCol(\"filtered\")\n",
    "cleaned = remover.transform(transformed)\n",
    "cleaned.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom stopwords\n",
    "stopwords = StopWordsRemover().getStopWords() + [\"-\"]\n",
    "remover = StopWordsRemover().setStopWords(stopwords).setInputCol(\"words\").setOutputCol(\"filtered\")\n",
    "cleaned = remover.transform(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate features\n",
    "from pyspark.ml.feature import CountVectorizer, CountVectorizerModel\n",
    "cvmodel = CountVectorizer().setInputCol(\"filtered\").setOutputCol(\"features\").fit(cleaned)\n",
    "featured = cvmodel.transform(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to binary label\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "indexer = StringIndexer().setInputCol(\"spam\").setOutputCol(\"label\").fit(featured)\n",
    "indexed = indexer.transform(featured)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|spam|             message|               words|            filtered|            features|label|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "| ham| &lt;#&gt;  in mc...|[, &lt;#&gt;, , i...|[, &lt;#&gt;, , m...|(13422,[3,6,3909,...|  0.0|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split to train and test\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "training, test = indexed.randomSplit([0.7, 0.3], seed = 12345)\n",
    "training.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n",
      "|            features|label|prediction|\n",
      "+--------------------+-----+----------+\n",
      "|(13422,[3,11,159,...|  0.0|       0.0|\n",
      "|(13422,[3,12,77,8...|  0.0|       0.0|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "Accuracy 0.5\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "lrModel = lr.fit(training)\n",
    "predictions = lrModel.transform(test)\n",
    "predictions.select(\"features\", \"label\", \"prediction\").show(2)\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator().setLabelCol(\"label\").setRawPredictionCol(\"prediction\").setMetricName(\"areaUnderROC\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print (\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.5138248847926268\n"
     ]
    }
   ],
   "source": [
    "# Random Forest check the accuracy in data\n",
    "from pyspark.ml.classification import RandomForestClassificationModel, RandomForestClassifier\n",
    "rf = RandomForestClassifier().setLabelCol(\"label\").setFeaturesCol(\"features\").setNumTrees(10)\n",
    "model = rf.fit(training)\n",
    "predictions = model.transform(test)\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator().setLabelCol(\"label\").setRawPredictionCol(\"prediction\").setMetricName(\"areaUnderROC\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print (\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|ngrams                                                                                                                                                                                       |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[go jurong, jurong point,, point, crazy.., crazy.. available, available bugis, bugis n, n great, great world, world la, la e, e buffet..., buffet... cine, cine got, got amore, amore wat...]|\n",
      "|[ok lar..., lar... joking, joking wif, wif u, u oni...]                                                                                                                                      |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "ngram = NGram().setN(2).setInputCol(\"filtered\").setOutputCol(\"ngrams\")\n",
    "ngramDataFrame = ngram.transform(cleaned)\n",
    "ngramDataFrame.select(\"ngrams\").show(2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "tokenizer = Tokenizer().setInputCol(\"message\").setOutputCol(\"words\")\n",
    "\n",
    "stopwords = StopWordsRemover().getStopWords()+ [\"-\"]\n",
    "remover = StopWordsRemover().setStopWords(stopwords).setInputCol(\"words\").setOutputCol(\"filtered\")\n",
    "cvmodel = CountVectorizer().setInputCol(\"filtered\").setOutputCol(\"features\")\n",
    "indexer = StringIndexer().setInputCol(\"spam\").setOutputCol(\"label\")\n",
    "lr = LogisticRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "pipeline = Pipeline().setStages([tokenizer, remover, cvmodel, indexer, lr])\n",
    "model = pipeline.fit(raw)\n",
    "model.write().overwrite().save(\"demos/spam_model4.4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = PipelineModel.load(\"demos/spam_model4.4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
